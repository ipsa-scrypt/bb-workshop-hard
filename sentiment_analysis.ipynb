{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Welcome !\n",
    "\n",
    "> Let's see what we can do together. Let's take a look at some sentiment analysis.\n",
    "\n",
    "***To detect good sentences from bad we will use a labeled dataset of 3000 sentences.***\n",
    "<br>\n",
    "<img src=\"https://images7.alphacoders.com/132/1325363.png\" alt=\"Mario Star\" width=\"500\"/>\n",
    "\n",
    "### What is it?\n",
    "Let's say we have for example: **\"I love this cat\"** and **\"I love this dog\"**, **\"I hate the movie\"**\n",
    "<br>\n",
    "We can see that the first two sentences are positive and the last one is negative. Therefore, we can say:\n",
    "**I love this cat** is **1** and **I hate the movie** is **0**.\n",
    "<br>\n",
    "We deal with 2 emotions: **positive** and **negative**. We can also say that we work in a one-dimensional space: **0** to **1**.\n",
    "\n",
    "\n",
    "| When you see a dragon ball, it means it's your time to shine and to code!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8264d63c770c4779"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Take a look at the data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1359b800bec3ce72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_data() -> list:\n",
    "    path: str = os.path.join(os.getcwd(), 'dataset')\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError('Dataset not found')\n",
    "    labeled_sentences: list = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(path, filename), 'r') as f:\n",
    "                for line in f:\n",
    "                    labeled_sentences.append(line.strip().split('\\t'))\n",
    "    return labeled_sentences"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2554d66a27e0a2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_dataframe(labeled_sentences: list) -> pd.DataFrame:\n",
    "    df: pd.DataFrame = pd.DataFrame(labeled_sentences, columns=['sentence', 'label'])\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a1acb8f4ea84e121"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = get_dataframe(read_data())\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3791b116380e70be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5d67041cf9d7aa68"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data():\n",
    "    sentences: list = df['sentence'].values.tolist()\n",
    "    labels: list = df['label'].values.tolist()\n",
    "    return sentences, labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ddde04241d0eb3e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sentences, labels = preprocess_data()\n",
    "print(sentences[:1])\n",
    "print(labels[:1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb63f27279a64491"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tokenization and Padding\n",
    "\n",
    "**Let's code!** <img src=\"https://i.pinimg.com/originals/88/3c/ac/883cacfc0a39afa24693ac441e5bdbec.png\" width=\"200\"/> **Let's feed the dragon!**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d45b4ebdc24f554c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the `tf.keras.preprocessing.text.Tokenizer` to tokenize the sentences.\n",
    "Then, we want to fit the tokenizer on the sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50808072935188cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(sentences: list) -> tf.keras.preprocessing.text.Tokenizer:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cc9b811ae667bf6a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to pad the sentences to the same length. We will use the median of the lengths of the sentences."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c60bf4d8659c610"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def padding() -> int:\n",
    "    lengths: list = [len(sentence.split()) for sentence in sentences]\n",
    "    percentiles: list = []\n",
    "    for p in [75, 80, 85, 90, 95, 99,  100]:\n",
    "        percentiles.append([p, np.percentile(lengths, p)])\n",
    "    median: int = (percentiles[-1][1] + percentiles[-2][1]) // 2\n",
    "    return int(median)\n",
    "\n",
    "def create_dataset_from_tokenizer(tokenizer: tf.keras.preprocessing.text.Tokenizer, pad=int) -> tf.data.Dataset:\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=pad)\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "    labels_float = tf.strings.to_number(labels, out_type=tf.float32)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((padded_sequences, labels_float))\n",
    "    return dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2b1c0f846e68bad7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Now, let's use the above functions to create our consumable dataset: our AI cannot eat \"strings\" or \"lists\" but it can eat \"tensors\"!\n",
    "\n",
    "To do so:\n",
    "(1) We tokenize the sentences\n",
    "(2) We pad the sentences\n",
    "(3) We create a dataset from the tokenizer\n",
    "(4) We shuffle the dataset (`dataset.shuffle`)\n",
    "(5) We split the dataset into 3 parts: train, validation and test"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75e5a21e577b18eb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Data Splitting!\n",
    "\n",
    "# TODO: (1) tokenize the sentences\n",
    "\n",
    "# TODO: (2) pad the sentences\n",
    "\n",
    "# TODO: (3) create a dataset from the tokenizer\n",
    "# dataset = \n",
    "# TODO: (4) shuffle the dataset\n",
    "\n",
    "# TODO: (5) split the dataset into 3 parts: train, validation and test\n",
    "# -> train: 80%, validation: 10%, test: 10% for example\n",
    "\n",
    "# train_dataset =\n",
    "# val_dataset =\n",
    "# test_dataset ="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "67519c665d8e7fe1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Train size: {len(list(train_dataset))}')\n",
    "print(f'Validation size: {len(list(val_dataset))}')\n",
    "print(f'Test size: {len(list(test_dataset))}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50e38f1dd5f3bc85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5f180fefe774d23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a batch size of 64 and 10 epochs.\n",
    "Batch the datasets: `dataset.batch(batch_size)`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc80785a319cef4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: Add the hyperparameters: batch_size and epochs\n",
    "\n",
    "# batch the datasets\n",
    "# train_dataset = \n",
    "# val_dataset = \n",
    "# test_dataset = "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7257d0d80ac912f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Train size: {len(list(train_dataset))}')\n",
    "print(f'Validation size: {len(list(val_dataset))}')\n",
    "print(f'Test size: {len(list(test_dataset))}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30beea1718135d1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuring the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d94ceb9cd0004d5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "logs_dir = os.path.join(os.getcwd(), 'logs')\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.mkdir(logs_dir)\n",
    "data_dir = os.path.join(logs_dir, 'data')\n",
    "if not os.path.exists(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "# save the data for later use, callbacks\n",
    "best_model_path = os.path.join(data_dir, 'best_model')\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True)\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "callbacks = [checkpoint, early_stopping, tensorboard]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ccb48bbf86a02a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "598d5ae41a6841c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use a `Bidirectional LSTM` with `64` units and a `Dense` layer with `64` units.\n",
    "Here is the model summary:\n",
    "- Embedding: `tokenizer.word_index` + 1, `pad` (padding)\n",
    "- Bidirectional LSTM: `pad`\n",
    "- Dense: `64`, `relu`\n",
    "- Dense: `1`, `sigmoid` (output)\n",
    "\n",
    "We will use the `binary_crossentropy` loss function and the `adam` optimizer to compile the model.\n",
    "You can observe the metrics: `accuracy`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7b2e3fef42492d8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model() -> tf.keras.models.Sequential:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "932b8794665cd1fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17863e93e10ff985"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4386385a10b17c96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will train the model on the train dataset and validate it on the validation dataset.\n",
    "Don't forget to use the callbacks!\n",
    "`history = model.fit(...)`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cad80c7f95ea20e7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# history = "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c1ab097c913a1e2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac9a2fb484c50bf8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_from_best():\n",
    "    \"\"\"\n",
    "    :return: loss, accuracy\n",
    "    \"\"\"\n",
    "    best_model_path = os.path.join(os.getcwd(), 'logs', 'data', 'best_model')\n",
    "    model.load_weights(best_model_path)\n",
    "    loss, accuracy = model.evaluate(test_dataset, batch_size=batch_size)\n",
    "    print('Loss: {}, Accuracy: {}'.format(loss, accuracy))\n",
    "\n",
    "evaluate_from_best()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2867f7eac44d7932"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save plot in plots/ directory\n",
    "plots_dir = os.path.join(os.getcwd(), 'plots')\n",
    "if not os.path.exists(plots_dir):\n",
    "    os.mkdir(plots_dir)\n",
    "accuracy_plot_path = os.path.join(plots_dir, 'accuracy_plot.png')\n",
    "loss_plot_path = os.path.join(plots_dir, 'loss_plot.png')\n",
    "\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.savefig(accuracy_plot_path)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend(loc='upper right')\n",
    "plt.savefig(loss_plot_path)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c304484c0db1958c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict(sentence: str) -> float:\n",
    "    \"\"\"\n",
    "    :param sentence: sentence to predict\n",
    "    :return: prediction\n",
    "    \"\"\"\n",
    "    if isinstance(sentence, str):\n",
    "        sequences = tokenizer.texts_to_sequences([sentence])\n",
    "        padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
    "        prediction = model.predict(padded_sequences)\n",
    "        prediction = [1 if p > 0.5 else 0 for p in prediction]\n",
    "        return prediction\n",
    "    else:\n",
    "        raise TypeError('Sentence must be a string')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "738981ab437cbb7b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Positive\n",
    "sentence = 'I love you'\n",
    "prediction = predict(sentence)\n",
    "print(f\"Sentence: {sentence} \\nPrediction: {':) UwU'  if prediction[0] == 1 else ':('}\")\n",
    "\n",
    "# Negative\n",
    "sentence = 'I won\\'t go there!'\n",
    "prediction = predict(sentence)\n",
    "print(f\"Sentence: {sentence} \\nPrediction: {':) UwU'  if prediction[0] == 1 else ':('}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20aaa9b1f084a699"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
